{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa9dc912",
   "metadata": {},
   "source": [
    "# Project: Kigali Traffic Congestion Prediction\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "### 1.1 Project Overview\n",
    "\n",
    "This project focuses on developing machine learning models to predict traffic congestion risk at urban intersections. Efficient traffic management is critical for rapidly urbanizing cities like Kigali, as congestion impacts economic productivity, environmental quality, and daily urban mobility.\n",
    "\n",
    "### 1.2 Dataset Context\n",
    "\n",
    "This project utilizes a Kaggle competition dataset comprising aggregated trip logging metrics from commercial vehicles. This dataset provides detailed information on vehicle stoppages and delays at intersections within a major urban area (e.g., North America).\n",
    "\n",
    "**Dataset Rationale:**\n",
    "* **Relevance:** The dataset directly addresses the problem of traffic congestion prediction and provides rich, real-world metrics (time stopped, distance to stop) essential for this task.\n",
    "* **Complexity:** It offers a non-trivial challenge, requiring careful feature engineering and robust model development. This aligns with the assignment's objective to move beyond generic use cases.\n",
    "* **Transferability:** The methodologies and insights gained from this project, utilizing this dataset, are directly applicable to traffic management challenges in other urban environments, including Kigali, given the availability of similar data. This project serves as a prototype demonstrating the application of advanced ML techniques for urban mobility.\n",
    "\n",
    "## 2. Data Acquisition and Initial Exploration\n",
    "\n",
    "This section covers loading the dataset and performing an initial examination to understand its structure, content, and statistical properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1255b51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded.\n",
      "\n",
      "First 5 rows of DataFrame:\n",
      "     RowId  IntersectionId   Latitude  Longitude  \\\n",
      "0  1921357               0  33.791659 -84.430032   \n",
      "1  1921358               0  33.791659 -84.430032   \n",
      "2  1921359               0  33.791659 -84.430032   \n",
      "3  1921360               0  33.791659 -84.430032   \n",
      "4  1921361               0  33.791659 -84.430032   \n",
      "\n",
      "                EntryStreetName                ExitStreetName EntryHeading  \\\n",
      "0  Marietta Boulevard Northwest  Marietta Boulevard Northwest           NW   \n",
      "1  Marietta Boulevard Northwest  Marietta Boulevard Northwest           SE   \n",
      "2  Marietta Boulevard Northwest  Marietta Boulevard Northwest           NW   \n",
      "3  Marietta Boulevard Northwest  Marietta Boulevard Northwest           SE   \n",
      "4  Marietta Boulevard Northwest  Marietta Boulevard Northwest           NW   \n",
      "\n",
      "  ExitHeading  Hour  Weekend  ...  TimeFromFirstStop_p40  \\\n",
      "0          NW     0        0  ...                    0.0   \n",
      "1          SE     0        0  ...                    0.0   \n",
      "2          NW     1        0  ...                    0.0   \n",
      "3          SE     1        0  ...                    0.0   \n",
      "4          NW     2        0  ...                    0.0   \n",
      "\n",
      "  TimeFromFirstStop_p50  TimeFromFirstStop_p60  TimeFromFirstStop_p80  \\\n",
      "0                   0.0                    0.0                    0.0   \n",
      "1                   0.0                    0.0                    0.0   \n",
      "2                   0.0                    0.0                    0.0   \n",
      "3                   0.0                    0.0                    0.0   \n",
      "4                   0.0                    0.0                    0.0   \n",
      "\n",
      "   DistanceToFirstStop_p20  DistanceToFirstStop_p40  DistanceToFirstStop_p50  \\\n",
      "0                      0.0                      0.0                      0.0   \n",
      "1                      0.0                      0.0                      0.0   \n",
      "2                      0.0                      0.0                      0.0   \n",
      "3                      0.0                      0.0                      0.0   \n",
      "4                      0.0                      0.0                      0.0   \n",
      "\n",
      "   DistanceToFirstStop_p60  DistanceToFirstStop_p80     City  \n",
      "0                      0.0                      0.0  Atlanta  \n",
      "1                      0.0                      0.0  Atlanta  \n",
      "2                      0.0                      0.0  Atlanta  \n",
      "3                      0.0                      0.0  Atlanta  \n",
      "4                      0.0                      0.0  Atlanta  \n",
      "\n",
      "[5 rows x 28 columns]\n",
      "\n",
      "DataFrame Information:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 856387 entries, 0 to 856386\n",
      "Data columns (total 28 columns):\n",
      " #   Column                   Non-Null Count   Dtype  \n",
      "---  ------                   --------------   -----  \n",
      " 0   RowId                    856387 non-null  int64  \n",
      " 1   IntersectionId           856387 non-null  int64  \n",
      " 2   Latitude                 856387 non-null  float64\n",
      " 3   Longitude                856387 non-null  float64\n",
      " 4   EntryStreetName          848239 non-null  object \n",
      " 5   ExitStreetName           850100 non-null  object \n",
      " 6   EntryHeading             856387 non-null  object \n",
      " 7   ExitHeading              856387 non-null  object \n",
      " 8   Hour                     856387 non-null  int64  \n",
      " 9   Weekend                  856387 non-null  int64  \n",
      " 10  Month                    856387 non-null  int64  \n",
      " 11  Path                     856387 non-null  object \n",
      " 12  TotalTimeStopped_p20     856387 non-null  float64\n",
      " 13  TotalTimeStopped_p40     856387 non-null  float64\n",
      " 14  TotalTimeStopped_p50     856387 non-null  float64\n",
      " 15  TotalTimeStopped_p60     856387 non-null  float64\n",
      " 16  TotalTimeStopped_p80     856387 non-null  float64\n",
      " 17  TimeFromFirstStop_p20    856387 non-null  float64\n",
      " 18  TimeFromFirstStop_p40    856387 non-null  float64\n",
      " 19  TimeFromFirstStop_p50    856387 non-null  float64\n",
      " 20  TimeFromFirstStop_p60    856387 non-null  float64\n",
      " 21  TimeFromFirstStop_p80    856387 non-null  float64\n",
      " 22  DistanceToFirstStop_p20  856387 non-null  float64\n",
      " 23  DistanceToFirstStop_p40  856387 non-null  float64\n",
      " 24  DistanceToFirstStop_p50  856387 non-null  float64\n",
      " 25  DistanceToFirstStop_p60  856387 non-null  float64\n",
      " 26  DistanceToFirstStop_p80  856387 non-null  float64\n",
      " 27  City                     856387 non-null  object \n",
      "dtypes: float64(17), int64(5), object(6)\n",
      "memory usage: 182.9+ MB\n",
      "\n",
      "Descriptive Statistics for Numerical Features:\n",
      "              RowId  IntersectionId       Latitude      Longitude  \\\n",
      "count  8.563870e+05   856387.000000  856387.000000  856387.000000   \n",
      "mean   2.349550e+06      833.283384      39.618965     -77.916488   \n",
      "std    2.472178e+05      654.308913       2.935437       5.952959   \n",
      "min    1.921357e+06        0.000000      33.649973     -87.862288   \n",
      "25%    2.135454e+06      291.000000      39.936739     -84.387607   \n",
      "50%    2.349550e+06      679.000000      39.982974     -75.175055   \n",
      "75%    2.563646e+06     1264.000000      41.910047     -75.100495   \n",
      "max    2.777743e+06     2875.000000      42.381782     -71.025550   \n",
      "\n",
      "                Hour        Weekend          Month  TotalTimeStopped_p20  \\\n",
      "count  856387.000000  856387.000000  856387.000000         856387.000000   \n",
      "mean       12.431234       0.277880       9.104808              1.755596   \n",
      "std         6.071843       0.447954       1.991094              7.146549   \n",
      "min         0.000000       0.000000       1.000000              0.000000   \n",
      "25%         8.000000       0.000000       7.000000              0.000000   \n",
      "50%        13.000000       0.000000       9.000000              0.000000   \n",
      "75%        17.000000       1.000000      11.000000              0.000000   \n",
      "max        23.000000       1.000000      12.000000            298.000000   \n",
      "\n",
      "       TotalTimeStopped_p40  TotalTimeStopped_p50  ...  TimeFromFirstStop_p20  \\\n",
      "count         856387.000000         856387.000000  ...          856387.000000   \n",
      "mean               5.403592              7.722655  ...               3.181096   \n",
      "std               12.981674             15.685910  ...              11.835994   \n",
      "min                0.000000              0.000000  ...               0.000000   \n",
      "25%                0.000000              0.000000  ...               0.000000   \n",
      "50%                0.000000              0.000000  ...               0.000000   \n",
      "75%                0.000000             10.000000  ...               0.000000   \n",
      "max              375.000000            375.000000  ...             337.000000   \n",
      "\n",
      "       TimeFromFirstStop_p40  TimeFromFirstStop_p50  TimeFromFirstStop_p60  \\\n",
      "count          856387.000000          856387.000000          856387.000000   \n",
      "mean                9.162174              12.722165              18.926085   \n",
      "std                20.446568              24.219271              29.851797   \n",
      "min                 0.000000               0.000000               0.000000   \n",
      "25%                 0.000000               0.000000               0.000000   \n",
      "50%                 0.000000               0.000000               0.000000   \n",
      "75%                 0.000000              22.000000              31.000000   \n",
      "max               356.000000             356.000000             357.000000   \n",
      "\n",
      "       TimeFromFirstStop_p80  DistanceToFirstStop_p20  \\\n",
      "count          856387.000000            856387.000000   \n",
      "mean               34.201656                 6.765856   \n",
      "std                41.130668                29.535968   \n",
      "min                 0.000000                 0.000000   \n",
      "25%                 0.000000                 0.000000   \n",
      "50%                27.000000                 0.000000   \n",
      "75%                49.000000                 0.000000   \n",
      "max               359.000000              1901.900000   \n",
      "\n",
      "       DistanceToFirstStop_p40  DistanceToFirstStop_p50  \\\n",
      "count            856387.000000            856387.000000   \n",
      "mean                 20.285128                28.837113   \n",
      "std                  59.202108                75.217343   \n",
      "min                   0.000000                 0.000000   \n",
      "25%                   0.000000                 0.000000   \n",
      "50%                   0.000000                 0.000000   \n",
      "75%                   0.000000                53.100000   \n",
      "max                2844.400000              2851.100000   \n",
      "\n",
      "       DistanceToFirstStop_p60  DistanceToFirstStop_p80  \n",
      "count             856387.00000            856387.000000  \n",
      "mean                  44.27231                83.991313  \n",
      "std                  102.03225               160.709797  \n",
      "min                    0.00000                 0.000000  \n",
      "25%                    0.00000                 0.000000  \n",
      "50%                    0.00000                60.400000  \n",
      "75%                   64.20000                85.950000  \n",
      "max                 3282.40000              4079.200000  \n",
      "\n",
      "[8 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, log_loss, roc_auc_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Load the training dataset\n",
    "# The 'train.csv' file is expected to be in the same directory as this notebook.\n",
    "train_df = pd.read_csv('train.csv')\n",
    "\n",
    "print(\"Dataset loaded.\")\n",
    "\n",
    "# Display initial rows and dataset information\n",
    "print(\"\\nFirst 5 rows of DataFrame:\")\n",
    "print(train_df.head())\n",
    "\n",
    "print(\"\\nDataFrame Information:\")\n",
    "train_df.info()\n",
    "\n",
    "print(\"\\nDescriptive Statistics for Numerical Features:\")\n",
    "print(train_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa50ba4",
   "metadata": {},
   "source": [
    "### 2.2 Feature Examination and Initial Cleaning\n",
    "\n",
    "This step involves identifying relevant features, checking data types, and handling missing values to prepare the dataset for feature engineering.\n",
    "\n",
    "**Key Features:**\n",
    "* **`IntersectionId`, `Latitude`, `Longitude`**: Spatial identifiers. `Latitude` and `Longitude` will be used as primary spatial features.\n",
    "* **`Hour`, `Weekend`, `Month`**: Temporal indicators. These will be transformed to capture cyclical patterns.\n",
    "* **`TotalTimeStopped_pXX`, `TimeFromFirstStop_pXX`, `DistanceToFirstStop_pXX`**: Percentile-based metrics indicating vehicle stop times and distances. These are central to defining congestion and deriving new features.\n",
    "* **`count`**: Represents the volume of vehicles in an observation group.\n",
    "\n",
    "**Initial Cleaning:**\n",
    "* Unnecessary identifier columns (`RowId`, `IntersectionId`) will be dropped.\n",
    "* Numerical columns will be ensured to have correct data types, and any `NaN` values will be imputed (e.g., with 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ec0c33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Diagnosing Remaining NaNs ---\n",
      "No NaN values found in any columns. Good to go!\n",
      "\n",
      "DataFrame Info after NaN fix attempt:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 856387 entries, 0 to 856386\n",
      "Data columns (total 26 columns):\n",
      " #   Column                   Non-Null Count   Dtype  \n",
      "---  ------                   --------------   -----  \n",
      " 0   Latitude                 856387 non-null  float64\n",
      " 1   Longitude                856387 non-null  float64\n",
      " 2   EntryStreetName          856387 non-null  float64\n",
      " 3   ExitStreetName           856387 non-null  float64\n",
      " 4   EntryHeading             856387 non-null  object \n",
      " 5   ExitHeading              856387 non-null  object \n",
      " 6   Hour                     856387 non-null  int64  \n",
      " 7   Weekend                  856387 non-null  int64  \n",
      " 8   Month                    856387 non-null  int64  \n",
      " 9   Path                     856387 non-null  object \n",
      " 10  TotalTimeStopped_p20     856387 non-null  float64\n",
      " 11  TotalTimeStopped_p40     856387 non-null  float64\n",
      " 12  TotalTimeStopped_p50     856387 non-null  float64\n",
      " 13  TotalTimeStopped_p60     856387 non-null  float64\n",
      " 14  TotalTimeStopped_p80     856387 non-null  float64\n",
      " 15  TimeFromFirstStop_p20    856387 non-null  float64\n",
      " 16  TimeFromFirstStop_p40    856387 non-null  float64\n",
      " 17  TimeFromFirstStop_p50    856387 non-null  float64\n",
      " 18  TimeFromFirstStop_p60    856387 non-null  float64\n",
      " 19  TimeFromFirstStop_p80    856387 non-null  float64\n",
      " 20  DistanceToFirstStop_p20  856387 non-null  float64\n",
      " 21  DistanceToFirstStop_p40  856387 non-null  float64\n",
      " 22  DistanceToFirstStop_p50  856387 non-null  float64\n",
      " 23  DistanceToFirstStop_p60  856387 non-null  float64\n",
      " 24  DistanceToFirstStop_p80  856387 non-null  float64\n",
      " 25  City                     856387 non-null  object \n",
      "dtypes: float64(19), int64(3), object(4)\n",
      "memory usage: 169.9+ MB\n",
      "\n",
      "Descriptive Statistics after NaN fix attempt:\n",
      "            Latitude      Longitude  EntryStreetName  ExitStreetName  \\\n",
      "count  856387.000000  856387.000000         856387.0        856387.0   \n",
      "mean       39.618965     -77.916488              0.0             0.0   \n",
      "std         2.935437       5.952959              0.0             0.0   \n",
      "min        33.649973     -87.862288              0.0             0.0   \n",
      "25%        39.936739     -84.387607              0.0             0.0   \n",
      "50%        39.982974     -75.175055              0.0             0.0   \n",
      "75%        41.910047     -75.100495              0.0             0.0   \n",
      "max        42.381782     -71.025550              0.0             0.0   \n",
      "\n",
      "                Hour        Weekend          Month  TotalTimeStopped_p20  \\\n",
      "count  856387.000000  856387.000000  856387.000000         856387.000000   \n",
      "mean       12.431234       0.277880       9.104808              1.755596   \n",
      "std         6.071843       0.447954       1.991094              7.146549   \n",
      "min         0.000000       0.000000       1.000000              0.000000   \n",
      "25%         8.000000       0.000000       7.000000              0.000000   \n",
      "50%        13.000000       0.000000       9.000000              0.000000   \n",
      "75%        17.000000       1.000000      11.000000              0.000000   \n",
      "max        23.000000       1.000000      12.000000            298.000000   \n",
      "\n",
      "       TotalTimeStopped_p40  TotalTimeStopped_p50  ...  TimeFromFirstStop_p20  \\\n",
      "count         856387.000000         856387.000000  ...          856387.000000   \n",
      "mean               5.403592              7.722655  ...               3.181096   \n",
      "std               12.981674             15.685910  ...              11.835994   \n",
      "min                0.000000              0.000000  ...               0.000000   \n",
      "25%                0.000000              0.000000  ...               0.000000   \n",
      "50%                0.000000              0.000000  ...               0.000000   \n",
      "75%                0.000000             10.000000  ...               0.000000   \n",
      "max              375.000000            375.000000  ...             337.000000   \n",
      "\n",
      "       TimeFromFirstStop_p40  TimeFromFirstStop_p50  TimeFromFirstStop_p60  \\\n",
      "count          856387.000000          856387.000000          856387.000000   \n",
      "mean                9.162174              12.722165              18.926085   \n",
      "std                20.446568              24.219271              29.851797   \n",
      "min                 0.000000               0.000000               0.000000   \n",
      "25%                 0.000000               0.000000               0.000000   \n",
      "50%                 0.000000               0.000000               0.000000   \n",
      "75%                 0.000000              22.000000              31.000000   \n",
      "max               356.000000             356.000000             357.000000   \n",
      "\n",
      "       TimeFromFirstStop_p80  DistanceToFirstStop_p20  \\\n",
      "count          856387.000000            856387.000000   \n",
      "mean               34.201656                 6.765856   \n",
      "std                41.130668                29.535968   \n",
      "min                 0.000000                 0.000000   \n",
      "25%                 0.000000                 0.000000   \n",
      "50%                27.000000                 0.000000   \n",
      "75%                49.000000                 0.000000   \n",
      "max               359.000000              1901.900000   \n",
      "\n",
      "       DistanceToFirstStop_p40  DistanceToFirstStop_p50  \\\n",
      "count            856387.000000            856387.000000   \n",
      "mean                 20.285128                28.837113   \n",
      "std                  59.202108                75.217343   \n",
      "min                   0.000000                 0.000000   \n",
      "25%                   0.000000                 0.000000   \n",
      "50%                   0.000000                 0.000000   \n",
      "75%                   0.000000                53.100000   \n",
      "max                2844.400000              2851.100000   \n",
      "\n",
      "       DistanceToFirstStop_p60  DistanceToFirstStop_p80  \n",
      "count             856387.00000            856387.000000  \n",
      "mean                  44.27231                83.991313  \n",
      "std                  102.03225               160.709797  \n",
      "min                    0.00000                 0.000000  \n",
      "25%                    0.00000                 0.000000  \n",
      "50%                    0.00000                60.400000  \n",
      "75%                   64.20000                85.950000  \n",
      "max                 3282.40000              4079.200000  \n",
      "\n",
      "[8 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Diagnosing Remaining NaNs ---\")\n",
    "# Check which columns still have NaNs and how many\n",
    "nan_counts = train_df.isnull().sum()\n",
    "columns_with_nan = nan_counts[nan_counts > 0]\n",
    "\n",
    "if not columns_with_nan.empty:\n",
    "    print(\"Columns still containing NaN values:\")\n",
    "    print(columns_with_nan)\n",
    "\n",
    "    # Let's re-apply a more thorough NaN handling for ALL numerical columns\n",
    "    print(\"\\nAttempting to re-fill NaNs in all numerical columns with 0...\")\n",
    "    for col in columns_with_nan.index:\n",
    "        if train_df[col].dtype != np.number: # Try converting to numeric first if it's not already\n",
    "            train_df[col] = pd.to_numeric(train_df[col], errors='coerce')\n",
    "        train_df[col] = train_df[col].fillna(0) # Then fill NaNs\n",
    "\n",
    "    print(\"\\n--- Re-checking NaN values after re-filling ---\")\n",
    "    final_nan_check = train_df.isnull().sum().sum()\n",
    "    print(f\"Total NaN values remaining in DataFrame: {final_nan_check}\")\n",
    "\n",
    "    if final_nan_check == 0:\n",
    "        print(\"All NaN values have been successfully filled! Proceeding with confidence.\")\n",
    "    else:\n",
    "        print(\"Warning: Some NaN values still remain. Further investigation needed.\")\n",
    "else:\n",
    "    print(\"No NaN values found in any columns. Good to go!\")\n",
    "\n",
    "# Let's re-display the info and describe to confirm types and no NaNs\n",
    "print(\"\\nDataFrame Info after NaN fix attempt:\")\n",
    "train_df.info()\n",
    "\n",
    "print(\"\\nDescriptive Statistics after NaN fix attempt:\")\n",
    "print(train_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df33629b",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "This section details the creation of the target variable and the engineering of new features from existing raw data to enhance model learning.\n",
    "\n",
    "### 3.1 Defining the `Congested` Target Variable\n",
    "\n",
    "The dataset does not contain an explicit 'congested' label. A binary target variable, `Congested` (1 for congested, 0 for not congested), will be engineered based on the `TotalTimeStopped_p50` (median total time stopped) metric. A threshold is applied to this metric to classify congestion.\n",
    "\n",
    "**Threshold Selection:** A threshold of 45 seconds for `TotalTimeStopped_p50` is selected as a preliminary indicator of congestion. This value can be adjusted based on subsequent model performance analysis or domain insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37b80e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congestion target variable created (Threshold: 30 seconds).\n",
      "\n",
      "Distribution of 'Congested' (1) vs. 'Not Congested' (0) labels:\n",
      "Congested\n",
      "0    780648\n",
      "1     75739\n",
      "Name: count, dtype: int64\n",
      "Congested\n",
      "0    0.91156\n",
      "1    0.08844\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Define the threshold for median total time stopped to classify an intersection as 'Congested'.\n",
    "# For example, if median time stopped is 45 seconds or more, it's considered congested.\n",
    "CONGESTION_THRESHOLD_SECONDS = 30\n",
    "\n",
    "# Create the 'Congested' target column: 1 if median stop time meets threshold, else 0.\n",
    "train_df['Congested'] = (train_df['TotalTimeStopped_p50'] >= CONGESTION_THRESHOLD_SECONDS).astype(int)\n",
    "\n",
    "print(f\"Congestion target variable created (Threshold: {CONGESTION_THRESHOLD_SECONDS} seconds).\")\n",
    "print(\"\\nDistribution of 'Congested' (1) vs. 'Not Congested' (0) labels:\")\n",
    "print(train_df['Congested'].value_counts())\n",
    "print(train_df['Congested'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a011d2",
   "metadata": {},
   "source": [
    "### 3.2 Enhanced Temporal and Statistical Features\n",
    "\n",
    "New features are engineered to provide more comprehensive information to the models:\n",
    "\n",
    "* **Cyclical Temporal Features:** `Hour` and `Month` represent cyclical phenomena. Sine and cosine transformations are applied to these features. This method allows models to correctly interpret the proximity of values across a cycle (e.g., 23:00 being near 0:00), which is crucial for capturing daily and seasonal patterns in traffic.\n",
    "* **Derived Statistical Features from Percentiles:** The `TotalTimeStopped_pXX` columns provide percentile values of total time stopped. To summarize this distribution, the mean and range across these percentiles are calculated. These derived features offer insights into the average congestion severity and its variability, enriching the feature set beyond individual percentile values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fd55e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cyclical temporal features added; original 'Hour' and 'Month' columns removed.\n",
      "Derived statistical features (mean and range of TotalTimeStopped percentiles) added; other specific percentile columns mostly removed.\n",
      "Interaction features (lat_x_lon, lat_plus_lon) added.\n"
     ]
    }
   ],
   "source": [
    "# Create cyclical features for 'Hour' and 'Month'.\n",
    "train_df['hour_sin'] = np.sin(2 * np.pi * train_df['Hour'] / 24.0)\n",
    "train_df['hour_cos'] = np.cos(2 * np.pi * train_df['Hour'] / 24.0)\n",
    "train_df['month_sin'] = np.sin(2 * np.pi * train_df['Month'] / 12.0)\n",
    "train_df['month_cos'] = np.cos(2 * np.pi * train_df['Month'] / 12.0)\n",
    "\n",
    "# Drop original 'Hour' and 'Month' columns as their cyclical representations are now included.\n",
    "train_df.drop(['Hour', 'Month'], axis=1, inplace=True)\n",
    "print(\"Cyclical temporal features added; original 'Hour' and 'Month' columns removed.\")\n",
    "\n",
    "# Define the base metric for which percentile statistics will be calculated.\n",
    "# Focus on 'TotalTimeStopped' as it is the most direct indicator of congestion for these derived features.\n",
    "metric_to_process = 'TotalTimeStopped'\n",
    "percentiles_suffix = ['p20', 'p40', 'p50', 'p60', 'p80']\n",
    "cols_for_metric = [f\"{metric_to_process}_{p}\" for p in percentiles_suffix]\n",
    "\n",
    "# Calculate the mean of percentiles for 'TotalTimeStopped'.\n",
    "train_df[f'{metric_to_process}_mean_pctl'] = train_df[cols_for_metric].mean(axis=1)\n",
    "# Calculate the range (max - min) of percentiles for 'TotalTimeStopped'.\n",
    "train_df[f'{metric_to_process}_range_pctl'] = train_df[cols_for_metric].max(axis=1) - train_df[cols_for_metric].min(axis=1)\n",
    "\n",
    "# Drop original individual percentile columns related to TotalTimeStopped,\n",
    "# except 'TotalTimeStopped_p50' which is used for the target variable.\n",
    "for col in cols_for_metric:\n",
    "    if col != 'TotalTimeStopped_p50':\n",
    "        if col in train_df.columns:\n",
    "            train_df.drop(col, axis=1, inplace=True)\n",
    "\n",
    "# Drop all percentile columns for 'TimeFromFirstStop' and 'DistanceToFirstStop'\n",
    "# to simplify the feature set, relying on 'TotalTimeStopped' derived features.\n",
    "percentile_cols_to_drop = [col for col in train_df.columns if ('TimeFromFirstStop_p' in col or 'DistanceToFirstStop_p' in col)]\n",
    "train_df.drop(percentile_cols_to_drop, axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "print(\"Derived statistical features (mean and range of TotalTimeStopped percentiles) added; other specific percentile columns mostly removed.\")\n",
    "\n",
    "# Create simple interaction features from Latitude and Longitude.\n",
    "train_df['lat_x_lon'] = train_df['Latitude'] * train_df['Longitude']\n",
    "train_df['lat_plus_lon'] = train_df['Latitude'] + train_df['Longitude']\n",
    "print(\"Interaction features (lat_x_lon, lat_plus_lon) added.\")\n",
    "\n",
    "# Ensure 'count' column is numerical and handle any remaining NaNs for safety.\n",
    "if 'count' in train_df.columns:\n",
    "    train_df['count'] = pd.to_numeric(train_df['count'], errors='coerce').fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a6b7ab",
   "metadata": {},
   "source": [
    "## 4. Data Splitting and Scaling\n",
    "\n",
    "Data preparation for model training involves partitioning the dataset into distinct sets and scaling numerical features.\n",
    "\n",
    "* **Data Splitting**: The dataset is divided into three sets:\n",
    "    * **Training Set**: Used for model parameter learning.\n",
    "    * **Validation Set**: Used for hyperparameter tuning and early stopping during Neural Network training to prevent overfitting.\n",
    "    * **Test Set**: Reserved for a final, unbiased evaluation of the best-performing model on unseen data. A 70% train, 15% validation, and 15% test split ratio is applied, using stratified sampling to maintain class proportions.\n",
    "* **Feature Scaling (`StandardScaler`)**: Numerical features are scaled using `StandardScaler`. This transforms data to have a mean of 0 and a standard deviation of 1. Scaling is crucial for Neural Networks and beneficial for many other machine learning algorithms, as it standardizes feature magnitudes and prevents features with larger ranges from dominating the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92cc82b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final features selected for modeling: ['Latitude', 'Longitude', 'EntryStreetName', 'ExitStreetName', 'Weekend', 'TotalTimeStopped_p50', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos', 'TotalTimeStopped_mean_pctl', 'TotalTimeStopped_range_pctl', 'lat_x_lon', 'lat_plus_lon']\n",
      "Total number of features: 14\n",
      "\n",
      "Dataset shapes after splitting:\n",
      "X_train shape: (599470, 14)\n",
      "y_train shape: (599470,)\n",
      "X_val shape: (128458, 14)\n",
      "y_val shape: (128458,)\n",
      "X_test shape: (128459, 14)\n",
      "y_test shape: (128459,)\n",
      "\n",
      "Target variable distribution in each split:\n",
      "Training set distribution:\n",
      "Congested\n",
      "0    0.91156\n",
      "1    0.08844\n",
      "Name: proportion, dtype: float64\n",
      "Validation set distribution:\n",
      "Congested\n",
      "0    0.911559\n",
      "1    0.088441\n",
      "Name: proportion, dtype: float64\n",
      "Test set distribution:\n",
      "Congested\n",
      "0    0.911559\n",
      "1    0.088441\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Features scaled using StandardScaler.\n",
      "Data preparation complete.\n"
     ]
    }
   ],
   "source": [
    "# Define the feature set (X) and target variable (y).\n",
    "features = [col for col in train_df.columns if col not in ['Congested']]\n",
    "target = 'Congested'\n",
    "\n",
    "X = train_df[features]\n",
    "y = train_df[target]\n",
    "\n",
    "# Ensure all feature columns are numerical and handle any remaining NaNs.\n",
    "X = X.select_dtypes(include=np.number)\n",
    "X.fillna(0, inplace=True)\n",
    "\n",
    "print(f\"Final features selected for modeling: {list(X.columns)}\")\n",
    "print(f\"Total number of features: {len(X.columns)}\")\n",
    "\n",
    "# Split data into training, validation, and test sets.\n",
    "# First, split off the test set (15% of the total).\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n",
    "\n",
    "# Then, split the remaining 'X_train_val' into training and validation sets.\n",
    "# The validation set will be 15% of the total dataset, calculated as a proportion of 'X_train_val'.\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=(0.15 / 0.85), random_state=42, stratify=y_train_val)\n",
    "\n",
    "print(f\"\\nDataset shapes after splitting:\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")\n",
    "print(f\"y_val shape: {y_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "print(f\"\\nTarget variable distribution in each split:\")\n",
    "print(f\"Training set distribution:\\n{y_train.value_counts(normalize=True)}\")\n",
    "print(f\"Validation set distribution:\\n{y_val.value_counts(normalize=True)}\")\n",
    "print(f\"Test set distribution:\\n{y_test.value_counts(normalize=True)}\")\n",
    "\n",
    "# Initialize StandardScaler for feature scaling.\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler exclusively on the training data to prevent data leakage.\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform the validation and test sets using the scaler fitted on training data.\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert scaled NumPy arrays back to Pandas DataFrames for consistent handling.\n",
    "# This uses the original column names and DataFrame indices.\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X.columns, index=X_train.index)\n",
    "X_val_scaled_df = pd.DataFrame(X_val_scaled, columns=X.columns, index=X_val.index)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X.columns, index=X_test.index)\n",
    "\n",
    "print(\"\\nFeatures scaled using StandardScaler.\")\n",
    "print(\"Data preparation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b4b91b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
