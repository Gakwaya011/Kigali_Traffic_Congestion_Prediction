{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa9dc912",
   "metadata": {},
   "source": [
    "# Project: Kigali Traffic Congestion Prediction\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "### 1.1 Project Overview\n",
    "\n",
    "This project focuses on developing machine learning models to predict traffic congestion risk at urban intersections. Efficient traffic management is critical for rapidly urbanizing cities like Kigali, as congestion impacts economic productivity, environmental quality, and daily urban mobility.\n",
    "\n",
    "### 1.2 Dataset Context\n",
    "\n",
    "This project utilizes a Kaggle competition dataset comprising aggregated trip logging metrics from commercial vehicles. This dataset provides detailed information on vehicle stoppages and delays at intersections within a major urban area (e.g., North America).\n",
    "\n",
    "**Dataset Rationale:**\n",
    "* **Relevance:** The dataset directly addresses the problem of traffic congestion prediction and provides rich, real-world metrics (time stopped, distance to stop) essential for this task.\n",
    "* **Complexity:** It offers a non-trivial challenge, requiring careful feature engineering and robust model development. This aligns with the assignment's objective to move beyond generic use cases.\n",
    "* **Transferability:** The methodologies and insights gained from this project, utilizing this dataset, are directly applicable to traffic management challenges in other urban environments, including Kigali, given the availability of similar data. This project serves as a prototype demonstrating the application of advanced ML techniques for urban mobility.\n",
    "\n",
    "## 2. Data Acquisition and Initial Exploration\n",
    "\n",
    "This section covers loading the dataset and performing an initial examination to understand its structure, content, and statistical properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1255b51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-17 17:16:46.373667: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-17 17:16:47.915461: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-17 17:16:48.768770: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750173409.478405  107008 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750173409.708166  107008 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1750173411.052317  107008 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750173411.052361  107008 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750173411.052366  107008 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750173411.052370  107008 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-17 17:16:51.192744: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully.\n",
      "\n",
      "First 5 rows of DataFrame:\n",
      "     RowId  IntersectionId   Latitude  Longitude  \\\n",
      "0  1921357               0  33.791659 -84.430032   \n",
      "1  1921358               0  33.791659 -84.430032   \n",
      "2  1921359               0  33.791659 -84.430032   \n",
      "3  1921360               0  33.791659 -84.430032   \n",
      "4  1921361               0  33.791659 -84.430032   \n",
      "\n",
      "                EntryStreetName                ExitStreetName EntryHeading  \\\n",
      "0  Marietta Boulevard Northwest  Marietta Boulevard Northwest           NW   \n",
      "1  Marietta Boulevard Northwest  Marietta Boulevard Northwest           SE   \n",
      "2  Marietta Boulevard Northwest  Marietta Boulevard Northwest           NW   \n",
      "3  Marietta Boulevard Northwest  Marietta Boulevard Northwest           SE   \n",
      "4  Marietta Boulevard Northwest  Marietta Boulevard Northwest           NW   \n",
      "\n",
      "  ExitHeading  Hour  Weekend  ...  TimeFromFirstStop_p40  \\\n",
      "0          NW     0        0  ...                    0.0   \n",
      "1          SE     0        0  ...                    0.0   \n",
      "2          NW     1        0  ...                    0.0   \n",
      "3          SE     1        0  ...                    0.0   \n",
      "4          NW     2        0  ...                    0.0   \n",
      "\n",
      "  TimeFromFirstStop_p50  TimeFromFirstStop_p60  TimeFromFirstStop_p80  \\\n",
      "0                   0.0                    0.0                    0.0   \n",
      "1                   0.0                    0.0                    0.0   \n",
      "2                   0.0                    0.0                    0.0   \n",
      "3                   0.0                    0.0                    0.0   \n",
      "4                   0.0                    0.0                    0.0   \n",
      "\n",
      "   DistanceToFirstStop_p20  DistanceToFirstStop_p40  DistanceToFirstStop_p50  \\\n",
      "0                      0.0                      0.0                      0.0   \n",
      "1                      0.0                      0.0                      0.0   \n",
      "2                      0.0                      0.0                      0.0   \n",
      "3                      0.0                      0.0                      0.0   \n",
      "4                      0.0                      0.0                      0.0   \n",
      "\n",
      "   DistanceToFirstStop_p60  DistanceToFirstStop_p80     City  \n",
      "0                      0.0                      0.0  Atlanta  \n",
      "1                      0.0                      0.0  Atlanta  \n",
      "2                      0.0                      0.0  Atlanta  \n",
      "3                      0.0                      0.0  Atlanta  \n",
      "4                      0.0                      0.0  Atlanta  \n",
      "\n",
      "[5 rows x 28 columns]\n",
      "\n",
      "DataFrame Information:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 856387 entries, 0 to 856386\n",
      "Data columns (total 28 columns):\n",
      " #   Column                   Non-Null Count   Dtype  \n",
      "---  ------                   --------------   -----  \n",
      " 0   RowId                    856387 non-null  int64  \n",
      " 1   IntersectionId           856387 non-null  int64  \n",
      " 2   Latitude                 856387 non-null  float64\n",
      " 3   Longitude                856387 non-null  float64\n",
      " 4   EntryStreetName          848239 non-null  object \n",
      " 5   ExitStreetName           850100 non-null  object \n",
      " 6   EntryHeading             856387 non-null  object \n",
      " 7   ExitHeading              856387 non-null  object \n",
      " 8   Hour                     856387 non-null  int64  \n",
      " 9   Weekend                  856387 non-null  int64  \n",
      " 10  Month                    856387 non-null  int64  \n",
      " 11  Path                     856387 non-null  object \n",
      " 12  TotalTimeStopped_p20     856387 non-null  float64\n",
      " 13  TotalTimeStopped_p40     856387 non-null  float64\n",
      " 14  TotalTimeStopped_p50     856387 non-null  float64\n",
      " 15  TotalTimeStopped_p60     856387 non-null  float64\n",
      " 16  TotalTimeStopped_p80     856387 non-null  float64\n",
      " 17  TimeFromFirstStop_p20    856387 non-null  float64\n",
      " 18  TimeFromFirstStop_p40    856387 non-null  float64\n",
      " 19  TimeFromFirstStop_p50    856387 non-null  float64\n",
      " 20  TimeFromFirstStop_p60    856387 non-null  float64\n",
      " 21  TimeFromFirstStop_p80    856387 non-null  float64\n",
      " 22  DistanceToFirstStop_p20  856387 non-null  float64\n",
      " 23  DistanceToFirstStop_p40  856387 non-null  float64\n",
      " 24  DistanceToFirstStop_p50  856387 non-null  float64\n",
      " 25  DistanceToFirstStop_p60  856387 non-null  float64\n",
      " 26  DistanceToFirstStop_p80  856387 non-null  float64\n",
      " 27  City                     856387 non-null  object \n",
      "dtypes: float64(17), int64(5), object(6)\n",
      "memory usage: 182.9+ MB\n",
      "\n",
      "Descriptive Statistics for Numerical Features:\n",
      "              RowId  IntersectionId       Latitude      Longitude  \\\n",
      "count  8.563870e+05   856387.000000  856387.000000  856387.000000   \n",
      "mean   2.349550e+06      833.283384      39.618965     -77.916488   \n",
      "std    2.472178e+05      654.308913       2.935437       5.952959   \n",
      "min    1.921357e+06        0.000000      33.649973     -87.862288   \n",
      "25%    2.135454e+06      291.000000      39.936739     -84.387607   \n",
      "50%    2.349550e+06      679.000000      39.982974     -75.175055   \n",
      "75%    2.563646e+06     1264.000000      41.910047     -75.100495   \n",
      "max    2.777743e+06     2875.000000      42.381782     -71.025550   \n",
      "\n",
      "                Hour        Weekend          Month  TotalTimeStopped_p20  \\\n",
      "count  856387.000000  856387.000000  856387.000000         856387.000000   \n",
      "mean       12.431234       0.277880       9.104808              1.755596   \n",
      "std         6.071843       0.447954       1.991094              7.146549   \n",
      "min         0.000000       0.000000       1.000000              0.000000   \n",
      "25%         8.000000       0.000000       7.000000              0.000000   \n",
      "50%        13.000000       0.000000       9.000000              0.000000   \n",
      "75%        17.000000       1.000000      11.000000              0.000000   \n",
      "max        23.000000       1.000000      12.000000            298.000000   \n",
      "\n",
      "       TotalTimeStopped_p40  TotalTimeStopped_p50  ...  TimeFromFirstStop_p20  \\\n",
      "count         856387.000000         856387.000000  ...          856387.000000   \n",
      "mean               5.403592              7.722655  ...               3.181096   \n",
      "std               12.981674             15.685910  ...              11.835994   \n",
      "min                0.000000              0.000000  ...               0.000000   \n",
      "25%                0.000000              0.000000  ...               0.000000   \n",
      "50%                0.000000              0.000000  ...               0.000000   \n",
      "75%                0.000000             10.000000  ...               0.000000   \n",
      "max              375.000000            375.000000  ...             337.000000   \n",
      "\n",
      "       TimeFromFirstStop_p40  TimeFromFirstStop_p50  TimeFromFirstStop_p60  \\\n",
      "count          856387.000000          856387.000000          856387.000000   \n",
      "mean                9.162174              12.722165              18.926085   \n",
      "std                20.446568              24.219271              29.851797   \n",
      "min                 0.000000               0.000000               0.000000   \n",
      "25%                 0.000000               0.000000               0.000000   \n",
      "50%                 0.000000               0.000000               0.000000   \n",
      "75%                 0.000000              22.000000              31.000000   \n",
      "max               356.000000             356.000000             357.000000   \n",
      "\n",
      "       TimeFromFirstStop_p80  DistanceToFirstStop_p20  \\\n",
      "count          856387.000000            856387.000000   \n",
      "mean               34.201656                 6.765856   \n",
      "std                41.130668                29.535968   \n",
      "min                 0.000000                 0.000000   \n",
      "25%                 0.000000                 0.000000   \n",
      "50%                27.000000                 0.000000   \n",
      "75%                49.000000                 0.000000   \n",
      "max               359.000000              1901.900000   \n",
      "\n",
      "       DistanceToFirstStop_p40  DistanceToFirstStop_p50  \\\n",
      "count            856387.000000            856387.000000   \n",
      "mean                 20.285128                28.837113   \n",
      "std                  59.202108                75.217343   \n",
      "min                   0.000000                 0.000000   \n",
      "25%                   0.000000                 0.000000   \n",
      "50%                   0.000000                 0.000000   \n",
      "75%                   0.000000                53.100000   \n",
      "max                2844.400000              2851.100000   \n",
      "\n",
      "       DistanceToFirstStop_p60  DistanceToFirstStop_p80  \n",
      "count             856387.00000            856387.000000  \n",
      "mean                  44.27231                83.991313  \n",
      "std                  102.03225               160.709797  \n",
      "min                    0.00000                 0.000000  \n",
      "25%                    0.00000                 0.000000  \n",
      "50%                    0.00000                60.400000  \n",
      "75%                   64.20000                85.950000  \n",
      "max                 3282.40000              4079.200000  \n",
      "\n",
      "[8 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os # For creating directories\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# Set a random seed for reproducibility across all runs\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Create a directory to save models if it doesn't exist\n",
    "models_dir = 'saved_models'\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "    print(f\"Created directory: {models_dir}\")\n",
    "\n",
    "# Load the training dataset\n",
    "# Ensure 'train.csv' is located in the same directory as this notebook.\n",
    "train_df = pd.read_csv('train.csv')\n",
    "\n",
    "print(\"Dataset loaded successfully.\")\n",
    "\n",
    "# Display initial rows and dataset information\n",
    "print(\"\\nFirst 5 rows of DataFrame:\")\n",
    "print(train_df.head())\n",
    "\n",
    "print(\"\\nDataFrame Information:\")\n",
    "train_df.info()\n",
    "\n",
    "print(\"\\nDescriptive Statistics for Numerical Features:\")\n",
    "print(train_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa50ba4",
   "metadata": {},
   "source": [
    "### 2.2 Feature Examination and Initial Cleaning\n",
    "\n",
    "This step involves identifying relevant features, checking data types, and handling missing values to prepare the dataset for feature engineering.\n",
    "\n",
    "**Key Features:**\n",
    "* **`IntersectionId`, `Latitude`, `Longitude`**: Spatial identifiers. `Latitude` and `Longitude` will be used as primary spatial features.\n",
    "* **`Hour`, `Weekend`, `Month`**: Temporal indicators. These will be transformed to capture cyclical patterns.\n",
    "* **`TotalTimeStopped_pXX`, `TimeFromFirstStop_pXX`, `DistanceToFirstStop_pXX`**: Percentile-based metrics indicating vehicle stop times and distances. These are central to defining congestion and deriving new features.\n",
    "* **`count`**: Represents the volume of vehicles in an observation group.\n",
    "\n",
    "**Initial Cleaning:**\n",
    "* Unnecessary identifier columns (`RowId`, `IntersectionId`) will be dropped.\n",
    "* Numerical columns will be ensured to have correct data types, and any `NaN` values will be imputed (e.g., with 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ec0c33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting initial data cleaning...\n",
      "Dropped column: 'RowId'.\n",
      "Dropped column: 'IntersectionId'.\n",
      "Dropped column: 'EntryStreetName'.\n",
      "Dropped column: 'ExitStreetName'.\n",
      "Dropped column: 'EntryHeading'.\n",
      "Dropped column: 'ExitHeading'.\n",
      "\n",
      "All remaining columns processed for numerical conversion and NaN imputation.\n",
      "Total NaN values remaining in DataFrame: 0\n",
      "All NaN values have been successfully filled. Data is clean for numerical processing.\n",
      "\n",
      "DataFrame Info after cleaning:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 856387 entries, 0 to 856386\n",
      "Data columns (total 22 columns):\n",
      " #   Column                   Non-Null Count   Dtype  \n",
      "---  ------                   --------------   -----  \n",
      " 0   Latitude                 856387 non-null  float64\n",
      " 1   Longitude                856387 non-null  float64\n",
      " 2   Hour                     856387 non-null  int64  \n",
      " 3   Weekend                  856387 non-null  int64  \n",
      " 4   Month                    856387 non-null  int64  \n",
      " 5   Path                     856387 non-null  float64\n",
      " 6   TotalTimeStopped_p20     856387 non-null  float64\n",
      " 7   TotalTimeStopped_p40     856387 non-null  float64\n",
      " 8   TotalTimeStopped_p50     856387 non-null  float64\n",
      " 9   TotalTimeStopped_p60     856387 non-null  float64\n",
      " 10  TotalTimeStopped_p80     856387 non-null  float64\n",
      " 11  TimeFromFirstStop_p20    856387 non-null  float64\n",
      " 12  TimeFromFirstStop_p40    856387 non-null  float64\n",
      " 13  TimeFromFirstStop_p50    856387 non-null  float64\n",
      " 14  TimeFromFirstStop_p60    856387 non-null  float64\n",
      " 15  TimeFromFirstStop_p80    856387 non-null  float64\n",
      " 16  DistanceToFirstStop_p20  856387 non-null  float64\n",
      " 17  DistanceToFirstStop_p40  856387 non-null  float64\n",
      " 18  DistanceToFirstStop_p50  856387 non-null  float64\n",
      " 19  DistanceToFirstStop_p60  856387 non-null  float64\n",
      " 20  DistanceToFirstStop_p80  856387 non-null  float64\n",
      " 21  City                     856387 non-null  float64\n",
      "dtypes: float64(19), int64(3)\n",
      "memory usage: 143.7 MB\n",
      "\n",
      "Descriptive Statistics after cleaning:\n",
      "            Latitude      Longitude           Hour        Weekend  \\\n",
      "count  856387.000000  856387.000000  856387.000000  856387.000000   \n",
      "mean       39.618965     -77.916488      12.431234       0.277880   \n",
      "std         2.935437       5.952959       6.071843       0.447954   \n",
      "min        33.649973     -87.862288       0.000000       0.000000   \n",
      "25%        39.936739     -84.387607       8.000000       0.000000   \n",
      "50%        39.982974     -75.175055      13.000000       0.000000   \n",
      "75%        41.910047     -75.100495      17.000000       1.000000   \n",
      "max        42.381782     -71.025550      23.000000       1.000000   \n",
      "\n",
      "               Month      Path  TotalTimeStopped_p20  TotalTimeStopped_p40  \\\n",
      "count  856387.000000  856387.0         856387.000000         856387.000000   \n",
      "mean        9.104808       0.0              1.755596              5.403592   \n",
      "std         1.991094       0.0              7.146549             12.981674   \n",
      "min         1.000000       0.0              0.000000              0.000000   \n",
      "25%         7.000000       0.0              0.000000              0.000000   \n",
      "50%         9.000000       0.0              0.000000              0.000000   \n",
      "75%        11.000000       0.0              0.000000              0.000000   \n",
      "max        12.000000       0.0            298.000000            375.000000   \n",
      "\n",
      "       TotalTimeStopped_p50  TotalTimeStopped_p60  ...  TimeFromFirstStop_p40  \\\n",
      "count         856387.000000         856387.000000  ...          856387.000000   \n",
      "mean               7.722655             11.925195  ...               9.162174   \n",
      "std               15.685910             19.761325  ...              20.446568   \n",
      "min                0.000000              0.000000  ...               0.000000   \n",
      "25%                0.000000              0.000000  ...               0.000000   \n",
      "50%                0.000000              0.000000  ...               0.000000   \n",
      "75%               10.000000             18.000000  ...               0.000000   \n",
      "max              375.000000            377.000000  ...             356.000000   \n",
      "\n",
      "       TimeFromFirstStop_p50  TimeFromFirstStop_p60  TimeFromFirstStop_p80  \\\n",
      "count          856387.000000          856387.000000          856387.000000   \n",
      "mean               12.722165              18.926085              34.201656   \n",
      "std                24.219271              29.851797              41.130668   \n",
      "min                 0.000000               0.000000               0.000000   \n",
      "25%                 0.000000               0.000000               0.000000   \n",
      "50%                 0.000000               0.000000              27.000000   \n",
      "75%                22.000000              31.000000              49.000000   \n",
      "max               356.000000             357.000000             359.000000   \n",
      "\n",
      "       DistanceToFirstStop_p20  DistanceToFirstStop_p40  \\\n",
      "count            856387.000000            856387.000000   \n",
      "mean                  6.765856                20.285128   \n",
      "std                  29.535968                59.202108   \n",
      "min                   0.000000                 0.000000   \n",
      "25%                   0.000000                 0.000000   \n",
      "50%                   0.000000                 0.000000   \n",
      "75%                   0.000000                 0.000000   \n",
      "max                1901.900000              2844.400000   \n",
      "\n",
      "       DistanceToFirstStop_p50  DistanceToFirstStop_p60  \\\n",
      "count            856387.000000             856387.00000   \n",
      "mean                 28.837113                 44.27231   \n",
      "std                  75.217343                102.03225   \n",
      "min                   0.000000                  0.00000   \n",
      "25%                   0.000000                  0.00000   \n",
      "50%                   0.000000                  0.00000   \n",
      "75%                  53.100000                 64.20000   \n",
      "max                2851.100000               3282.40000   \n",
      "\n",
      "       DistanceToFirstStop_p80      City  \n",
      "count            856387.000000  856387.0  \n",
      "mean                 83.991313       0.0  \n",
      "std                 160.709797       0.0  \n",
      "min                   0.000000       0.0  \n",
      "25%                   0.000000       0.0  \n",
      "50%                  60.400000       0.0  \n",
      "75%                  85.950000       0.0  \n",
      "max                4079.200000       0.0  \n",
      "\n",
      "[8 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Code - Initial Cleaning Implementation\n",
    "\n",
    "print(\"Starting initial data cleaning...\")\n",
    "\n",
    "# Explicitly define columns to drop initially before any type conversions\n",
    "cols_to_drop_initial = ['RowId', 'IntersectionId', 'EntryStreetName', 'ExitStreetName', 'EntryHeading', 'ExitHeading']\n",
    "\n",
    "# Drop these columns if they exist in the DataFrame\n",
    "for col in cols_to_drop_initial:\n",
    "    if col in train_df.columns:\n",
    "        train_df.drop(col, axis=1, inplace=True)\n",
    "        print(f\"Dropped column: '{col}'.\")\n",
    "    else:\n",
    "        print(f\"Column '{col}' not found to drop.\")\n",
    "\n",
    "# Now, ensure all REMAINING columns are numerical and fill any NaNs.\n",
    "# This loop will now only process columns that are intended to be numerical features.\n",
    "for col in train_df.columns:\n",
    "    # Attempt to convert to numeric. If it fails (e.g., if a string somehow remained), it becomes NaN.\n",
    "    train_df[col] = pd.to_numeric(train_df[col], errors='coerce')\n",
    "    # Fill any NaNs that resulted from coercion or were originally present.\n",
    "    train_df[col] = train_df[col].fillna(0)\n",
    "\n",
    "print(\"\\nAll remaining columns processed for numerical conversion and NaN imputation.\")\n",
    "print(f\"Total NaN values remaining in DataFrame: {train_df.isnull().sum().sum()}\")\n",
    "\n",
    "if train_df.isnull().sum().sum() == 0:\n",
    "    print(\"All NaN values have been successfully filled. Data is clean for numerical processing.\")\n",
    "else:\n",
    "    print(\"Warning: Some NaN values still remain. Further investigation needed.\")\n",
    "\n",
    "print(\"\\nDataFrame Info after cleaning:\")\n",
    "train_df.info()\n",
    "\n",
    "print(\"\\nDescriptive Statistics after cleaning:\")\n",
    "print(train_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df33629b",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "This section details the creation of the target variable and the engineering of new features from existing raw data to enhance model learning.\n",
    "\n",
    "### 3.1 Defining the `Congested` Target Variable\n",
    "\n",
    "The dataset does not contain an explicit 'congested' label. A binary target variable, `Congested` (1 for congested, 0 for not congested), will be engineered based on the `TotalTimeStopped_p50` (median total time stopped) metric. A threshold is applied to this metric to classify congestion.\n",
    "\n",
    "**Threshold Selection:** A threshold of 45 seconds for `TotalTimeStopped_p50` is selected as a preliminary indicator of congestion. This value can be adjusted based on subsequent model performance analysis or domain insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37b80e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congestion target variable created (Threshold: 30 seconds).\n",
      "\n",
      "Distribution of 'Congested' (1) vs. 'Not Congested' (0) labels:\n",
      "Congested\n",
      "0    780648\n",
      "1     75739\n",
      "Name: count, dtype: int64\n",
      "Congested\n",
      "0    0.91156\n",
      "1    0.08844\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Performing stratified sampling to minimize dataset size...\n",
      "Dataset reduced to 770748 rows (10.0% of original).\n",
      "New distribution of 'Congested' labels after sampling:\n",
      "Congested\n",
      "0    0.91156\n",
      "1    0.08844\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Define the threshold for median total time stopped to classify an intersection as 'Congested'.\n",
    "# For example, if median time stopped is 45 seconds or more, it's considered congested.\n",
    "CONGESTION_THRESHOLD_SECONDS = 30\n",
    "\n",
    "# Create the 'Congested' target column: 1 if median stop time meets threshold, else 0.\n",
    "train_df['Congested'] = (train_df['TotalTimeStopped_p50'] >= CONGESTION_THRESHOLD_SECONDS).astype(int)\n",
    "\n",
    "print(f\"Congestion target variable created (Threshold: {CONGESTION_THRESHOLD_SECONDS} seconds).\")\n",
    "print(\"\\nDistribution of 'Congested' (1) vs. 'Not Congested' (0) labels:\")\n",
    "print(train_df['Congested'].value_counts())\n",
    "print(train_df['Congested'].value_counts(normalize=True))\n",
    "# --- START OF STRATIFIED SAMPLING CODE TO INSERT ---\n",
    "print(\"\\nPerforming stratified sampling to minimize dataset size...\")\n",
    "# It's crucial to sample AFTER the 'Congested' column is created\n",
    "# We'll sample 20% of the data. You can adjust 'sample_fraction' as needed (e.g., 0.1 for 10%, 0.3 for 30%).\n",
    "sample_fraction = 0.1 # Keep 20% of the data\n",
    "\n",
    "# Ensure train_test_split is imported. Add 'from sklearn.model_selection import train_test_split' to Cell 2 if not already there.\n",
    "# from sklearn.model_selection import train_test_split # If you need to import here, but better in Cell 2\n",
    "\n",
    "train_df_sampled, _ = train_test_split(\n",
    "    train_df,\n",
    "    test_size=sample_fraction, # This is the fraction we *keep*\n",
    "    stratify=train_df['Congested'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_df = train_df_sampled.copy() # Overwrite the original DataFrame with the sampled one\n",
    "print(f\"Dataset reduced to {len(train_df)} rows ({sample_fraction*100}% of original).\")\n",
    "print(\"New distribution of 'Congested' labels after sampling:\")\n",
    "print(train_df['Congested'].value_counts(normalize=True).round(5))\n",
    "# --- END OF STRATIFIED SAMPLING CODE TO INSERT ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a011d2",
   "metadata": {},
   "source": [
    "### 3.2 Enhanced Temporal and Statistical Features\n",
    "\n",
    "New features are engineered to provide more comprehensive information to the models:\n",
    "\n",
    "* **Cyclical Temporal Features:** `Hour` and `Month` represent cyclical phenomena. Sine and cosine transformations are applied to these features. This method allows models to correctly interpret the proximity of values across a cycle (e.g., 23:00 being near 0:00), which is crucial for capturing daily and seasonal patterns in traffic.\n",
    "* **Derived Statistical Features from Percentiles:** The `TotalTimeStopped_pXX` columns provide percentile values of total time stopped. To summarize this distribution, the mean and range across these percentiles are calculated. These derived features offer insights into the average congestion severity and its variability, enriching the feature set beyond individual percentile values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fd55e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cyclical temporal features added; original 'Hour' and 'Month' columns removed.\n",
      "Derived statistical features (mean and range of TotalTimeStopped percentiles) added; other specific percentile columns mostly removed.\n",
      "Interaction features (lat_x_lon, lat_plus_lon) added.\n"
     ]
    }
   ],
   "source": [
    "# Create cyclical features for 'Hour' and 'Month'.\n",
    "train_df['hour_sin'] = np.sin(2 * np.pi * train_df['Hour'] / 24.0)\n",
    "train_df['hour_cos'] = np.cos(2 * np.pi * train_df['Hour'] / 24.0)\n",
    "train_df['month_sin'] = np.sin(2 * np.pi * train_df['Month'] / 12.0)\n",
    "train_df['month_cos'] = np.cos(2 * np.pi * train_df['Month'] / 12.0)\n",
    "\n",
    "# Drop original 'Hour' and 'Month' columns as their cyclical representations are now included.\n",
    "train_df.drop(['Hour', 'Month'], axis=1, inplace=True)\n",
    "print(\"Cyclical temporal features added; original 'Hour' and 'Month' columns removed.\")\n",
    "\n",
    "# Define the base metric for which percentile statistics will be calculated.\n",
    "# Focus on 'TotalTimeStopped' as it is the most direct indicator of congestion for these derived features.\n",
    "metric_to_process = 'TotalTimeStopped'\n",
    "percentiles_suffix = ['p20', 'p40', 'p50', 'p60', 'p80']\n",
    "cols_for_metric = [f\"{metric_to_process}_{p}\" for p in percentiles_suffix]\n",
    "\n",
    "# Calculate the mean of percentiles for 'TotalTimeStopped'.\n",
    "train_df[f'{metric_to_process}_mean_pctl'] = train_df[cols_for_metric].mean(axis=1)\n",
    "# Calculate the range (max - min) of percentiles for 'TotalTimeStopped'.\n",
    "train_df[f'{metric_to_process}_range_pctl'] = train_df[cols_for_metric].max(axis=1) - train_df[cols_for_metric].min(axis=1)\n",
    "\n",
    "# Drop original individual percentile columns related to TotalTimeStopped,\n",
    "# except 'TotalTimeStopped_p50' which is used for the target variable.\n",
    "for col in cols_for_metric:\n",
    "    if col != 'TotalTimeStopped_p50':\n",
    "        if col in train_df.columns:\n",
    "            train_df.drop(col, axis=1, inplace=True)\n",
    "\n",
    "# Drop all percentile columns for 'TimeFromFirstStop' and 'DistanceToFirstStop'\n",
    "# to simplify the feature set, relying on 'TotalTimeStopped' derived features.\n",
    "percentile_cols_to_drop = [col for col in train_df.columns if ('TimeFromFirstStop_p' in col or 'DistanceToFirstStop_p' in col)]\n",
    "train_df.drop(percentile_cols_to_drop, axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "print(\"Derived statistical features (mean and range of TotalTimeStopped percentiles) added; other specific percentile columns mostly removed.\")\n",
    "\n",
    "# Create simple interaction features from Latitude and Longitude.\n",
    "train_df['lat_x_lon'] = train_df['Latitude'] * train_df['Longitude']\n",
    "train_df['lat_plus_lon'] = train_df['Latitude'] + train_df['Longitude']\n",
    "print(\"Interaction features (lat_x_lon, lat_plus_lon) added.\")\n",
    "\n",
    "# Ensure 'count' column is numerical and handle any remaining NaNs for safety.\n",
    "if 'count' in train_df.columns:\n",
    "    train_df['count'] = pd.to_numeric(train_df['count'], errors='coerce').fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a6b7ab",
   "metadata": {},
   "source": [
    "## 4. Data Splitting and Scaling\n",
    "\n",
    "Data preparation for model training involves partitioning the dataset into distinct sets and scaling numerical features.\n",
    "\n",
    "* **Data Splitting**: The dataset is divided into three sets:\n",
    "    * **Training Set**: Used for model parameter learning.\n",
    "    * **Validation Set**: Used for hyperparameter tuning and early stopping during Neural Network training to prevent overfitting.\n",
    "    * **Test Set**: Reserved for a final, unbiased evaluation of the best-performing model on unseen data. A 70% train, 15% validation, and 15% test split ratio is applied, using stratified sampling to maintain class proportions.\n",
    "* **Feature Scaling (`StandardScaler`)**: Numerical features are scaled using `StandardScaler`. This transforms data to have a mean of 0 and a standard deviation of 1. Scaling is crucial for Neural Networks and beneficial for many other machine learning algorithms, as it standardizes feature magnitudes and prevents features with larger ranges from dominating the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92cc82b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final features selected for modeling: ['Latitude', 'Longitude', 'Weekend', 'Path', 'City', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos', 'TotalTimeStopped_mean_pctl', 'TotalTimeStopped_range_pctl', 'lat_x_lon', 'lat_plus_lon']\n",
      "Total number of features: 13\n",
      "\n",
      "Dataset shapes after splitting:\n",
      "X_train shape: (539522, 13)\n",
      "y_train shape: (539522,)\n",
      "X_val shape: (115613, 13)\n",
      "y_val shape: (115613,)\n",
      "X_test shape: (115613, 13)\n",
      "y_test shape: (115613,)\n",
      "\n",
      "Target variable distribution in each split:\n",
      "Training set distribution:\n",
      "Congested\n",
      "0    0.911561\n",
      "1    0.088439\n",
      "Name: proportion, dtype: float64\n",
      "Validation set distribution:\n",
      "Congested\n",
      "0    0.911558\n",
      "1    0.088442\n",
      "Name: proportion, dtype: float64\n",
      "Test set distribution:\n",
      "Congested\n",
      "0    0.911558\n",
      "1    0.088442\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Features scaled using StandardScaler.\n",
      "Data preparation complete.\n"
     ]
    }
   ],
   "source": [
    "# Define the feature set (X) and target variable (y).\n",
    "# IMPORTANT: Explicitly exclude 'TotalTimeStopped_p50' as it was used to define 'Congested' to prevent data leakage.\n",
    "features = [col for col in train_df.columns if col not in ['Congested', 'TotalTimeStopped_p50']]\n",
    "target = 'Congested'\n",
    "\n",
    "X = train_df[features]\n",
    "y = train_df[target]\n",
    "\n",
    "# Ensure all feature columns are numerical and handle any remaining NaNs.\n",
    "X = X.select_dtypes(include=np.number)\n",
    "X.fillna(0, inplace=True)\n",
    "\n",
    "print(f\"Final features selected for modeling: {list(X.columns)}\")\n",
    "print(f\"Total number of features: {len(X.columns)}\")\n",
    "\n",
    "# Split data into training, validation, and test sets.\n",
    "# First, split off the test set (15% of the total).\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n",
    "\n",
    "# Then, split the remaining 'X_train_val' into training and validation sets.\n",
    "# The validation set will be 15% of the total dataset, calculated as a proportion of 'X_train_val'.\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=(0.15 / 0.85), random_state=42, stratify=y_train_val)\n",
    "\n",
    "print(f\"\\nDataset shapes after splitting:\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")\n",
    "print(f\"y_val shape: {y_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "print(f\"\\nTarget variable distribution in each split:\")\n",
    "print(f\"Training set distribution:\\n{y_train.value_counts(normalize=True)}\")\n",
    "print(f\"Validation set distribution:\\n{y_val.value_counts(normalize=True)}\")\n",
    "print(f\"Test set distribution:\\n{y_test.value_counts(normalize=True)}\")\n",
    "\n",
    "# Initialize StandardScaler for feature scaling.\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler exclusively on the training data to prevent data leakage.\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform the validation and test sets using the scaler fitted on training data.\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert scaled NumPy arrays back to Pandas DataFrames for consistent handling.\n",
    "# This uses the original column names and DataFrame indices.\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X.columns, index=X_train.index)\n",
    "X_val_scaled_df = pd.DataFrame(X_val_scaled, columns=X.columns, index=X_val.index)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X.columns, index=X_test.index)\n",
    "\n",
    "print(\"\\nFeatures scaled using StandardScaler.\")\n",
    "print(\"Data preparation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24e6a18",
   "metadata": {},
   "source": [
    "## 5. Model Implementation: Classical Machine Learning with Optimization\n",
    "\n",
    "This section implements a classical machine learning model, Logistic Regression, which is widely used for binary classification. To ensure optimal performance as required by the rubric, its hyperparameters will be tuned using `GridSearchCV`.\n",
    "\n",
    "### 5.1 Logistic Regression with Hyperparameter Tuning\n",
    "\n",
    "**Logistic Regression** is a linear model that estimates the probability of a binary outcome. Its simplicity and interpretability make it an excellent baseline.\n",
    "\n",
    "**Hyperparameter Tuning with `GridSearchCV`:**\n",
    "`GridSearchCV` systematically works through multiple combinations of parameter values, cross-validating each combination to determine which set of parameters yields the best performance. For Logistic Regression, key hyperparameters include:\n",
    "* `C`: Inverse of regularization strength. Smaller values specify stronger regularization.\n",
    "* `solver`: Algorithm to use in the optimization problem. Different solvers work better with different datasets and regularization types.\n",
    "* `penalty`: The type of regularization (L1 or L2) to apply.\n",
    "\n",
    "The model will be trained on the training set and the best hyperparameters will be selected based on performance on the validation set during the grid search (implicitly, as cross-validation is performed on the training data for the grid search itself, and then the best model is validated on `X_val`). Final evaluation metrics will be reported on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d3c94c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Logistic Regression model with Hyperparameter Tuning...\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/miniconda3/envs/deepclass/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/chris/miniconda3/envs/deepclass/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/chris/miniconda3/envs/deepclass/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/chris/miniconda3/envs/deepclass/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/chris/miniconda3/envs/deepclass/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/chris/miniconda3/envs/deepclass/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/chris/miniconda3/envs/deepclass/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/chris/miniconda3/envs/deepclass/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/chris/miniconda3/envs/deepclass/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Hyperparameter Tuning Complete.\n",
      "Best parameters found: {'C': 1, 'penalty': 'l2', 'solver': 'saga'}\n",
      "Best F1-score from Grid Search: 0.8683\n",
      "\n",
      "--- Evaluating Best Logistic Regression Model on Validation Set ---\n",
      "Validation Accuracy (LR): 0.9779\n",
      "Validation Precision (LR): 0.8952\n",
      "Validation Recall (LR): 0.8494\n",
      "Validation F1-Score (LR): 0.8717\n",
      "Validation ROC AUC (LR): 0.9952\n",
      "\n",
      "Validation Classification Report (LR):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99    105388\n",
      "           1       0.90      0.85      0.87     10225\n",
      "\n",
      "    accuracy                           0.98    115613\n",
      "   macro avg       0.94      0.92      0.93    115613\n",
      "weighted avg       0.98      0.98      0.98    115613\n",
      "\n",
      "\n",
      "Optimized Logistic Regression model saved to: saved_models/logistic_regression_optimized_model.pkl\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting Logistic Regression model with Hyperparameter Tuning...\")\n",
    "\n",
    "# Define the Logistic Regression model\n",
    "log_reg = LogisticRegression(random_state=42, max_iter=500) # Increased max_iter for convergence\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10], # Inverse of regularization strength\n",
    "    'solver': ['liblinear', 'saga'], # 'liblinear' supports L1/L2, 'saga' supports L1/L2 and is faster for large datasets\n",
    "    'penalty': ['l1', 'l2'] # Type of regularization\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "# Scoring is F1-score as it's robust to class imbalance and combines precision/recall.\n",
    "# cv=3 for 3-fold cross-validation during tuning.\n",
    "grid_search = GridSearchCV(estimator=log_reg, param_grid=param_grid,\n",
    "                           scoring='f1', cv=3, verbose=1, n_jobs=-1) # n_jobs=-1 uses all available CPU cores\n",
    "\n",
    "# Perform the grid search on the training data\n",
    "grid_search.fit(X_train_scaled_df, y_train)\n",
    "\n",
    "print(\"\\nLogistic Regression Hyperparameter Tuning Complete.\")\n",
    "print(f\"Best parameters found: {grid_search.best_params_}\")\n",
    "print(f\"Best F1-score from Grid Search: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Get the best model\n",
    "best_log_reg_model = grid_search.best_estimator_\n",
    "\n",
    "# --- Evaluate the Best Logistic Regression Model on Validation Set ---\n",
    "print(\"\\n--- Evaluating Best Logistic Regression Model on Validation Set ---\")\n",
    "\n",
    "y_val_pred_proba_lr = best_log_reg_model.predict_proba(X_val_scaled_df)[:, 1]\n",
    "y_val_pred_lr = best_log_reg_model.predict(X_val_scaled_df)\n",
    "\n",
    "accuracy_lr = accuracy_score(y_val, y_val_pred_lr)\n",
    "precision_lr = precision_score(y_val, y_val_pred_lr)\n",
    "recall_lr = recall_score(y_val, y_val_pred_lr)\n",
    "f1_lr = f1_score(y_val, y_val_pred_lr)\n",
    "roc_auc_lr = roc_auc_score(y_val, y_val_pred_proba_lr)\n",
    "\n",
    "print(f\"Validation Accuracy (LR): {accuracy_lr:.4f}\")\n",
    "print(f\"Validation Precision (LR): {precision_lr:.4f}\")\n",
    "print(f\"Validation Recall (LR): {recall_lr:.4f}\")\n",
    "print(f\"Validation F1-Score (LR): {f1_lr:.4f}\")\n",
    "print(f\"Validation ROC AUC (LR): {roc_auc_lr:.4f}\")\n",
    "\n",
    "print(\"\\nValidation Classification Report (LR):\")\n",
    "print(classification_report(y_val, y_val_pred_lr))\n",
    "\n",
    "# --- Save the best Logistic Regression Model ---\n",
    "import joblib # Scikit-learn models are typically saved with joblib\n",
    "\n",
    "model_path_lr = os.path.join(models_dir, 'logistic_regression_optimized_model.pkl')\n",
    "joblib.dump(best_log_reg_model, model_path_lr)\n",
    "print(f\"\\nOptimized Logistic Regression model saved to: {model_path_lr}\")\n",
    "\n",
    "# Store LR results for later comparison in the summary\n",
    "lr_results = {\n",
    "    'Model': 'Logistic Regression (Optimized)',\n",
    "    'Accuracy': accuracy_lr,\n",
    "    'F1-score': f1_lr,\n",
    "    'Precision': precision_lr,\n",
    "    'Recall': recall_lr,\n",
    "    'ROC AUC': roc_auc_lr\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99be3a19",
   "metadata": {},
   "source": [
    "## 6. Model Implementation: Neural Networks (Multiple Instances)\n",
    "\n",
    "This section implements several Artificial Neural Network (ANN) models with varying architectures and optimization techniques. This systematic approach allows for comparing the impact of different hyperparameters and optimization strategies on model performance, as required by the rubric's comparison table.\n",
    "\n",
    "For each instance, the model will be compiled with `binary_crossentropy` loss and monitored for `accuracy`, `precision`, `recall`, and `auc`.\n",
    "\n",
    "### Neural Network Model Instances for Comparison:\n",
    "\n",
    "The following Neural Network models will be trained and evaluated:\n",
    "\n",
    "* **Instance 1: Simple/Baseline Neural Network (No Explicit Optimization)**\n",
    "    * This model uses a basic architecture without specified optimizers (defaults to Adam with default learning rate), no dropout, no early stopping, and a fixed number of epochs. It serves as a baseline to demonstrate the performance before applying explicit optimization techniques.\n",
    "* **Instance 2: Optimized NN (Adam, Dropout, Early Stopping, ReduceLR)**\n",
    "    * This instance incorporates the Adam optimizer with a custom learning rate, Dropout layers for regularization, Early Stopping to prevent overfitting, and `ReduceLROnPlateau` for adaptive learning rate adjustment.\n",
    "* **Instance 3: Optimized NN (RMSprop, L2 Regularization, More Layers)**\n",
    "    * This instance explores a different optimizer (RMSprop), adds L2 regularization to hidden layers, and uses a slightly deeper architecture to see its effect on performance.\n",
    "* **Instance 4: Optimized NN (Adam, Different Learning Rate, More Dropout)**\n",
    "    * This instance uses the Adam optimizer but with a different learning rate and increased dropout rates to investigate their impact on model training and generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5d8dfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to compile, train, evaluate, and save NN models\n",
    "nn_results_table = [] # List to store results for the comparison table\n",
    "\n",
    "def train_and_evaluate_nn(model, X_train_scaled, y_train, X_val_scaled, y_val,\n",
    "                          model_name, epochs=100, batch_size=64, callbacks=None,\n",
    "                          optimizer_name=\"Adam (Default LR)\", regularizer_used=\"None\",\n",
    "                          early_stopping_used=\"No\", num_layers=\"Default\", learning_rate=\"Default\",\n",
    "                          dropout_rate=\"None\"):\n",
    "\n",
    "    print(f\"\\n--- Training {model_name} ---\")\n",
    "\n",
    "    # Compile the model if not already compiled\n",
    "    if not model.optimizer: # Check if optimizer is set (can be set outside for complex cases)\n",
    "         model.compile(optimizer='adam', # Default for baseline if not specified\n",
    "                       loss='binary_crossentropy',\n",
    "                       metrics=['accuracy',\n",
    "                                tf.keras.metrics.Precision(name='precision'),\n",
    "                                tf.keras.metrics.Recall(name='recall'),\n",
    "                                tf.keras.metrics.AUC(name='auc')])\n",
    "\n",
    "    history = model.fit(X_train_scaled, y_train,\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        validation_data=(X_val_scaled, y_val),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose=0) # Set to 0 to suppress verbose output in notebook, 1 for progress bars\n",
    "\n",
    "    print(f\"{model_name} training complete.\")\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    val_metrics = model.evaluate(X_val_scaled, y_val, verbose=0)\n",
    "    metrics_names = model.metrics_names\n",
    "    results = dict(zip(metrics_names, val_metrics))\n",
    "\n",
    "    print(f\"  Validation Loss: {results['loss']:.4f}\")\n",
    "    print(f\"  Validation Accuracy: {results['accuracy']:.4f}\")\n",
    "    print(f\"  Validation Precision: {results['precision']:.4f}\")\n",
    "    print(f\"  Validation Recall: {results['recall']:.4f}\")\n",
    "    print(f\"  Validation ROC AUC: {results['auc']:.4f}\")\n",
    "\n",
    "    # Save the model\n",
    "    model_path = os.path.join(models_dir, f'{model_name.lower().replace(\" \", \"_\")}.h5')\n",
    "    model.save(model_path)\n",
    "    print(f\"  Model saved to: {model_path}\")\n",
    "\n",
    "    # Store results for the table\n",
    "    nn_results_table.append({\n",
    "        'Training Instance': model_name,\n",
    "        'Optimizer Used': optimizer_name,\n",
    "        'Regularizer Used': regularizer_used,\n",
    "        'Epochs': len(history.history['loss']), # Actual epochs run due to early stopping\n",
    "        'Early Stopping': early_stopping_used,\n",
    "        'Number of Layers': num_layers,\n",
    "        'Learning Rate': learning_rate,\n",
    "        'Accuracy': results['accuracy'],\n",
    "        'F1-score': results['f1_score'] if 'f1_score' in results else (2 * results['precision'] * results['recall']) / (results['precision'] + results['recall'] + 1e-7), # Calculate F1 if not directly provided by Keras\n",
    "        'Precision': results['precision'],\n",
    "        'Recall': results['recall'],\n",
    "        'ROC AUC': results['auc'],\n",
    "        'Loss': results['loss']\n",
    "    })\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b07c4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Instance 1: Simple/Baseline Neural Network...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/miniconda3/envs/deepclass/lib/python3.10/site-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2025-06-17 17:55:04.898166: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training NN_Instance_1_Simple_Baseline ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib # For saving/loading models and scalers\n",
    "\n",
    "# Ensure the train_and_evaluate_nn function is defined in a preceding cell,\n",
    "# or included here for completeness if you prefer. Assuming it's defined.\n",
    "\n",
    "# Placeholder for train_and_evaluate_nn for this example to be runnable\n",
    "# Make sure your actual train_and_evaluate_nn is correctly defined elsewhere (e.g., Cell 7)\n",
    "def train_and_evaluate_nn(model, X_train, y_train, X_val, y_val, model_name, epochs, callbacks=None, optimizer_name=\"Default\", regularizer_used=\"None\", early_stopping_used=\"No\", num_layers=\"N/A\", learning_rate=\"N/A\", dropout_rate=\"N/A\"):\n",
    "    print(f\"--- Training {model_name} ---\")\n",
    "    history = model.fit(X_train, y_train, epochs=epochs, validation_data=(X_val, y_val), callbacks=callbacks, verbose=0)\n",
    "    print(f\"{model_name} training complete.\")\n",
    "\n",
    "    val_metrics = model.evaluate(X_val, y_val, verbose=0)\n",
    "    metrics_names = model.metrics_names\n",
    "\n",
    "    results = dict(zip(metrics_names, val_metrics))\n",
    "\n",
    "    # Calculate F1-score as it's not directly returned by model.evaluate\n",
    "    y_pred_val = (model.predict(X_val) > 0.5).astype(int)\n",
    "    results['f1_score'] = f1_score(y_val, y_pred_val)\n",
    "\n",
    "    print(f\"  Validation Loss: {results['loss']:.4f}\")\n",
    "    print(f\"  Validation Accuracy: {results['accuracy']:.4f}\")\n",
    "    print(f\"  Validation Precision: {results['precision']:.4f}\")\n",
    "    print(f\"  Validation Recall: {results['recall']:.4f}\")\n",
    "    print(f\"  Validation F1-Score: {results['f1_score']:.4f}\")\n",
    "    print(f\"  Validation AUC: {results['auc']:.4f}\")\n",
    "\n",
    "    # Store results for later comparison\n",
    "    # Assuming 'all_model_results' is a list defined in your notebook\n",
    "    # all_model_results.append({\n",
    "    #     'Model Name': model_name,\n",
    "    #     'Validation Loss': results['loss'],\n",
    "    #     'Validation Accuracy': results['accuracy'],\n",
    "    #     'Validation Precision': results['precision'],\n",
    "    #     'Validation Recall': results['recall'],\n",
    "    #     'Validation F1-Score': results['f1_score'],\n",
    "    #     'Validation AUC': results['auc'],\n",
    "    #     'Optimizer': optimizer_name,\n",
    "    #     'Regularizer': regularizer_used,\n",
    "    #     'Early Stopping': early_stopping_used,\n",
    "    #     'Number of Layers': num_layers,\n",
    "    #     'Learning Rate': learning_rate,\n",
    "    #     'Dropout Rate': dropout_rate\n",
    "    # })\n",
    "\n",
    "    return model, history\n",
    "\n",
    "\n",
    "# --- Instance 1: Simple/Baseline Neural Network ---\n",
    "# No explicit optimizer specified (will use Keras default Adam), no dropout, no early stopping.\n",
    "\n",
    "print(\"Building Instance 1: Simple/Baseline Neural Network...\")\n",
    "\n",
    "# Ensure X_train_scaled_df is available from preceding cells\n",
    "# For this example, let's assume it exists and has been created\n",
    "input_dim = X_train_scaled_df.shape[1] # This line should be present in your actual notebook\n",
    "\n",
    "model_simple_nn = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(input_dim,)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# For this instance, we let Keras default the optimizer and learning rate if not specified\n",
    "# explicitly in model.compile. Here we explicitly use 'adam' without a learning_rate parameter\n",
    "# to reflect the \"default\" as per the rubric for instance 1.\n",
    "model_simple_nn.compile(optimizer='adam',\n",
    "                        loss='binary_crossentropy',\n",
    "                        metrics=[\n",
    "                                tf.keras.metrics.Accuracy(name='accuracy'), # <-- THIS IS THE CORRECT LINE\n",
    "                                tf.keras.metrics.Precision(name='precision'),\n",
    "                                tf.keras.metrics.Recall(name='recall'),\n",
    "                                tf.keras.metrics.AUC(name='auc')])\n",
    "\n",
    "\n",
    "# Train and evaluate\n",
    "model_simple_nn, history_simple = train_and_evaluate_nn(\n",
    "    model_simple_nn, X_train_scaled_df, y_train, X_val_scaled_df, y_val,\n",
    "    model_name=\"NN_Instance_1_Simple_Baseline\",\n",
    "    epochs=50, # Fixed epochs as no early stopping\n",
    "    optimizer_name=\"Adam (Keras Default)\",\n",
    "    regularizer_used=\"None\",\n",
    "    early_stopping_used=\"No\",\n",
    "    num_layers=\"3 (Input+2 Hidden+Output)\",\n",
    "    learning_rate=\"Default\",\n",
    "    dropout_rate=\"None\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8c4599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Instance 2: Optimized NN (Adam, Dropout, Early Stopping, ReduceLR) ---\n",
    "# Similar to previous \"optimized\" model, now explicitly categorized.\n",
    "\n",
    "print(\"\\nBuilding Instance 2: Optimized NN (Adam, Dropout, Early Stopping, ReduceLR)...\")\n",
    "\n",
    "input_dim = X_train_scaled_df.shape[1]\n",
    "\n",
    "model_opt_nn_1 = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(input_dim,)),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile with Adam optimizer and custom learning rate\n",
    "model_opt_nn_1.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                       loss='binary_crossentropy',\n",
    "                       metrics=[\n",
    "                                tf.keras.metrics.Accuracy(name='accuracy'),\n",
    "                                tf.keras.metrics.Precision(name='precision'),\n",
    "                                tf.keras.metrics.Recall(name='recall'),\n",
    "                                tf.keras.metrics.AUC(name='auc')])\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping_opt1 = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr_opt1 = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)\n",
    "\n",
    "# Train and evaluate\n",
    "model_opt_nn_1, history_opt1 = train_and_evaluate_nn(\n",
    "    model_opt_nn_1, X_train_scaled_df, y_train, X_val_scaled_df, y_val,\n",
    "    model_name=\"NN_Instance_2_Adam_Dropout_ES\",\n",
    "    epochs=100,\n",
    "    callbacks=[early_stopping_opt1, reduce_lr_opt1],\n",
    "    optimizer_name=\"Adam\",\n",
    "    regularizer_used=\"Dropout\",\n",
    "    early_stopping_used=\"Yes\",\n",
    "    num_layers=\"4 (Input+3 Hidden+Output)\",\n",
    "    learning_rate=\"0.001 (Adjusted)\",\n",
    "    dropout_rate=\"0.3, 0.3, 0.2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a1c183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Instance 3: Optimized NN (RMSprop, L2 Regularization, More Layers) ---\n",
    "\n",
    "print(\"\\nBuilding Instance 3: Optimized NN (RMSprop, L2 Regularization, More Layers)...\")\n",
    "\n",
    "input_dim = X_train_scaled_df.shape[1]\n",
    "\n",
    "model_opt_nn_2 = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(input_dim,), kernel_regularizer=regularizers.l2(0.001)),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    Dense(16, activation='relu', kernel_regularizer=regularizers.l2(0.001)), # Added another layer\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile with RMSprop optimizer and custom learning rate\n",
    "model_opt_nn_2.compile(optimizer=RMSprop(learning_rate=0.0005), # Different optimizer and LR\n",
    "                       loss='binary_crossentropy',\n",
    "                       metrics=[\n",
    "                                tf.keras.metrics.Accuracy(name='accuracy'),\n",
    "                                tf.keras.metrics.Precision(name='precision'),\n",
    "                                tf.keras.metrics.Recall(name='recall'),\n",
    "                                tf.keras.metrics.AUC(name='auc')])\n",
    "\n",
    "# Define callbacks (using Early Stopping but not ReduceLR this time to show variation)\n",
    "early_stopping_opt2 = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "\n",
    "# Train and evaluate\n",
    "model_opt_nn_2, history_opt2 = train_and_evaluate_nn(\n",
    "    model_opt_nn_2, X_train_scaled_df, y_train, X_val_scaled_df, y_val,\n",
    "    model_name=\"NN_Instance_3_RMSprop_L2_ES_MoreLayers\",\n",
    "    epochs=150, # More epochs given deeper model and potential slower convergence\n",
    "    callbacks=[early_stopping_opt2],\n",
    "    optimizer_name=\"RMSprop\",\n",
    "    regularizer_used=\"L2 (0.001), Dropout (0.2)\",\n",
    "    early_stopping_used=\"Yes\",\n",
    "    num_layers=\"5 (Input+4 Hidden+Output)\",\n",
    "    learning_rate=\"0.0005\",\n",
    "    dropout_rate=\"0.2, 0.2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09785bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Instance 4: Optimized NN (Adam, Different Learning Rate, More Dropout) ---\n",
    "\n",
    "print(\"\\nBuilding Instance 4: Optimized NN (Adam, Different Learning Rate, More Dropout)...\")\n",
    "\n",
    "input_dim = X_train_scaled_df.shape[1]\n",
    "\n",
    "model_opt_nn_3 = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(input_dim,)), # Larger first layer\n",
    "    Dropout(0.4), # Increased dropout\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.4), # Increased dropout\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile with Adam optimizer and different (lower) learning rate\n",
    "model_opt_nn_3.compile(optimizer=Adam(learning_rate=0.0005), # Lower LR than Instance 2\n",
    "                       loss='binary_crossentropy',\n",
    "                       metrics=[\n",
    "                                tf.keras.metrics.Accuracy(name='accuracy'),\n",
    "                                tf.keras.metrics.Precision(name='precision'),\n",
    "                                tf.keras.metrics.Recall(name='recall'),\n",
    "                                tf.keras.metrics.AUC(name='auc')])\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping_opt3 = EarlyStopping(monitor='val_loss', patience=12, restore_best_weights=True)\n",
    "reduce_lr_opt3 = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=6, min_lr=0.00005) # Different ReduceLR params\n",
    "\n",
    "# Train and evaluate\n",
    "model_opt_nn_3, history_opt3 = train_and_evaluate_nn(\n",
    "    model_opt_nn_3, X_train_scaled_df, y_train, X_val_scaled_df, y_val,\n",
    "    model_name=\"NN_Instance_4_Adam_DiffLR_MoreDropout\",\n",
    "    epochs=120,\n",
    "    callbacks=[early_stopping_opt3, reduce_lr_opt3],\n",
    "    optimizer_name=\"Adam\",\n",
    "    regularizer_used=\"Dropout (0.4, 0.4, 0.3)\",\n",
    "    early_stopping_used=\"Yes\",\n",
    "    num_layers=\"4 (Input+3 Hidden+Output)\",\n",
    "    learning_rate=\"0.0005 (Adjusted)\",\n",
    "    dropout_rate=\"0.4, 0.4, 0.3\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de44828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Instance 5: Optimized NN (Adam, Deeper, Less Dropout, Slightly Higher LR) ---\n",
    "# This is to fulfill the 5-row table requirement more robustly (1 baseline + 4 optimized)\n",
    "\n",
    "print(\"\\nBuilding Instance 5: Optimized NN (Adam, Deeper, Less Dropout, Slightly Higher LR)...\")\n",
    "\n",
    "input_dim = X_train_scaled_df.shape[1]\n",
    "\n",
    "model_opt_nn_4 = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(input_dim,)),\n",
    "    Dropout(0.2), # Less dropout than Instance 4\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.2), # Less dropout than Instance 4\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.1),\n",
    "    Dense(32, activation='relu'), # One more layer than Instance 4\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile with Adam optimizer and slightly higher learning rate than Instance 4\n",
    "model_opt_nn_4.compile(optimizer=Adam(learning_rate=0.0015), # Slightly higher LR\n",
    "                       loss='binary_crossentropy',\n",
    "                       metrics=[\n",
    "                                tf.keras.metrics.Accuracy(name='accuracy'),\n",
    "                                tf.keras.metrics.Precision(name='precision'),\n",
    "                                tf.keras.metrics.Recall(name='recall'),\n",
    "                                tf.keras.metrics.AUC(name='auc')])\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping_opt4 = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr_opt4 = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)\n",
    "\n",
    "# Train and evaluate\n",
    "model_opt_nn_4, history_opt4 = train_and_evaluate_nn(\n",
    "    model_opt_nn_4, X_train_scaled_df, y_train, X_val_scaled_df, y_val,\n",
    "    model_name=\"NN_Instance_5_Adam_Deeper_LessDropout_HigherLR\",\n",
    "    epochs=100,\n",
    "    callbacks=[early_stopping_opt4, reduce_lr_opt4],\n",
    "    optimizer_name=\"Adam\",\n",
    "    regularizer_used=\"Dropout (0.2, 0.2, 0.1)\",\n",
    "    early_stopping_used=\"Yes\",\n",
    "    num_layers=\"5 (Input+4 Hidden+Output)\",\n",
    "    learning_rate=\"0.0015\",\n",
    "    dropout_rate=\"0.2, 0.2, 0.1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e398ba12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of NN results into a Pandas DataFrame for easy viewing\n",
    "nn_results_df = pd.DataFrame(nn_results_table)\n",
    "\n",
    "print(\"\\n--- Neural Network Model Comparison Table (Validation Set) ---\")\n",
    "print(nn_results_df.round(4).to_string()) # .to_string() to avoid truncation\n",
    "\n",
    "# Optional: Add LR results to the NN table for a comprehensive comparison\n",
    "# You might choose to keep this separate for the README, but good for internal comparison\n",
    "full_comparison_table = pd.concat([pd.DataFrame([lr_results]), nn_results_df], ignore_index=True)\n",
    "print(\"\\n--- Full Model Comparison Table (Classical ML + Neural Networks) ---\")\n",
    "print(full_comparison_table.round(4).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc60d0ea",
   "metadata": {},
   "source": [
    "## 7. Final Model Selection and Evaluation\n",
    "\n",
    "After training and evaluating multiple models on the validation set, the best-performing model is selected. This final selected model is then evaluated on the completely unseen test dataset to provide an unbiased estimate of its generalization performance.\n",
    "\n",
    "**Model Selection Criteria:**\n",
    "The model with the highest F1-score on the validation set is typically chosen, as F1-score provides a good balance between Precision and Recall, which is crucial for imbalanced classification tasks like congestion prediction. ROC AUC is also a strong indicator of overall classifier performance.\n",
    "\n",
    "This final evaluation on the test set simulates how the model would perform in a real-world scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95626cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Final Model Selection and Evaluation on Test Set ---\")\n",
    "\n",
    "# Determine the best model based on F1-score on the validation set.\n",
    "# Assuming higher F1 is better. We need to compare the best LR F1 vs. best NN F1.\n",
    "\n",
    "best_nn_f1 = nn_results_df['F1-score'].max()\n",
    "best_nn_model_name = nn_results_df.loc[nn_results_df['F1-score'].idxmax(), 'Training Instance']\n",
    "\n",
    "print(f\"Best Neural Network (Validation F1): {best_nn_model_name} with F1-score = {best_nn_f1:.4f}\")\n",
    "print(f\"Best Logistic Regression (Validation F1): {lr_results['F1-score']:.4f}\")\n",
    "\n",
    "# Select the overall best model (either LR or the best NN)\n",
    "if lr_results['F1-score'] >= best_nn_f1:\n",
    "    final_best_model_name = \"Optimized Logistic Regression\"\n",
    "    final_best_model = best_log_reg_model\n",
    "    X_test_for_final_model = X_test_scaled_df\n",
    "    is_nn = False\n",
    "    print(\"\\nSelected best model: Optimized Logistic Regression.\")\n",
    "else:\n",
    "    final_best_model_name = best_nn_model_name\n",
    "    # Load the best NN model\n",
    "    best_nn_model_path = os.path.join(models_dir, f'{best_nn_model_name.lower().replace(\" \", \"_\")}.h5')\n",
    "    final_best_model = load_model(best_nn_model_path)\n",
    "    X_test_for_final_model = X_test_scaled_df\n",
    "    is_nn = True\n",
    "    print(f\"\\nSelected best model: {final_best_model_name}.\")\n",
    "\n",
    "\n",
    "# Evaluate the final best model on the unseen test set\n",
    "if is_nn:\n",
    "    test_metrics = final_best_model.evaluate(X_test_for_final_model, y_test, verbose=0)\n",
    "    metrics_names = final_best_model.metrics_names\n",
    "    final_test_results = dict(zip(metrics_names, test_metrics))\n",
    "    final_test_results['f1_score'] = (2 * final_test_results['precision'] * final_test_results['recall']) / \\\n",
    "                                     (final_test_results['precision'] + final_test_results['recall'] + 1e-7)\n",
    "\n",
    "    print(f\"\\n--- Final Test Set Evaluation for {final_best_model_name} ---\")\n",
    "    print(f\"Test Loss: {final_test_results['loss']:.4f}\")\n",
    "    print(f\"Test Accuracy: {final_test_results['accuracy']:.4f}\")\n",
    "    print(f\"Test Precision: {final_test_results['precision']:.4f}\")\n",
    "    print(f\"Test Recall: {final_test_results['recall']:.4f}\")\n",
    "    print(f\"Test F1-Score: {final_test_results['f1_score']:.4f}\")\n",
    "    print(f\"Test ROC AUC: {final_test_results['auc']:.4f}\")\n",
    "\n",
    "else: # Classical ML model (Logistic Regression)\n",
    "    y_test_pred_proba = final_best_model.predict_proba(X_test_for_final_model)[:, 1]\n",
    "    y_test_pred = final_best_model.predict(X_test_for_final_model)\n",
    "\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    test_precision = precision_score(y_test, y_test_pred)\n",
    "    test_recall = recall_score(y_test, y_test_pred)\n",
    "    test_f1 = f1_score(y_test, y_test_pred)\n",
    "    test_roc_auc = roc_auc_score(y_test, y_test_pred_proba)\n",
    "\n",
    "    print(f\"\\n--- Final Test Set Evaluation for {final_best_model_name} ---\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Test Precision: {test_precision:.4f}\")\n",
    "    print(f\"Test Recall: {test_recall:.4f}\")\n",
    "    print(f\"Test F1-Score: {test_f1:.4f}\")\n",
    "    print(f\"Test ROC AUC: {test_roc_auc:.4f}\")\n",
    "    print(\"\\nTest Classification Report:\")\n",
    "    print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "# Note: The table for the README needs to be manually extracted from the 'nn_results_df' and 'lr_results'\n",
    "# and formatted into the required structure."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
